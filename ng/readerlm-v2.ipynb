{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "if '__file__' not in globals():\n",
    "    __file__, __name__ = globals()['__vsc_ipynb_file__'], '__ipynb__'\n",
    "    import types, sys; sys.modules['__ipynb__'] = types.ModuleType('__ipynb__')\n",
    "\n",
    "import sys, os\n",
    "if os.path.abspath('.') not in sys.path: sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "class text_color:\n",
    "    def __init__(self, *attrs): self.clrs = list(set(attrs))\n",
    "    def __ror__(self, obj): return f'\\33[{\";\".join(map(str, self.clrs))}m{str(obj)}\\33[0m'\n",
    "    @property\n",
    "    def bg(self): return text_color( next((c+70 for c in self.clrs if 30 <= c < 38), self.clrs[0]))\n",
    "\n",
    "bold, italic, underline, strike = [1,3,4,9]\n",
    "black,red,green,yellow,blue,magenta,cyan,white = [text_color(clr) for clr in range(30,38)]\n",
    "\n",
    "import inspect\n",
    "class cout:\n",
    "    def __ror__(self, obj): print(f'[{inspect.stack()[1].lineno}]'|green, obj)\n",
    "    def __call__(self, *args, **kwds): print(f'[{inspect.stack()[1].lineno}]'|green, *args, **kwds)\n",
    "out = cout()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "jjkim_key = os.getenv(\"JINA_API_JJKIM\")\n",
    "obj76_key = os.getenv(\"JINA_API_OBJECTS76\")\n",
    "\n",
    "from rich import print as pprint\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.expanduser(\"~/.config/rofi/.env\"))\n",
    "jjkim_key = os.getenv(\"JINA_API_KEY_JJKIM\")\n",
    "obj76_key = os.getenv(\"JINA_API_KEY_OBJ76\")\n",
    "jjkim_key, obj76_key,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# html to markdown\n",
    "```bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# model_path = snapshot_download(\n",
    "#     repo_id = \"jinaai/ReaderLM-v2\",\n",
    "#     token = os.environ.get(\"HF_TOKEN\"),\n",
    "#     local_dir = \"/mnt/hdd/huggingface/hub/ReaderLM-v2\",\n",
    "#     tqdm_class = tqdm\n",
    "# )\n",
    "# print(f\"모델이 다운로드되었습니다: {model_path}\")\n",
    "\n",
    "outdir = \"/mnt/hdd/huggingface/hub/ReaderLM-v2/\"\n",
    "src = \"https://huggingface.co/jinaai/ReaderLM-v2/resolve/main\"\n",
    "filenames = [\n",
    "    \"README.md\",\n",
    "    \"added_tokens.json\",\n",
    "    \"config.json\",\n",
    "    \"generation_config.json\",\n",
    "    \"merges.txt\",\n",
    "    \"model.safetensors\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"vocab.json\"\n",
    "]\n",
    "for filename in filenames:\n",
    "    out_path = os.path.join(outdir, filename)\n",
    "    if not os.path.exists(out_path):\n",
    "        !wget {src}/{filename} -O {out_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = Path(\"/mnt/hdd/huggingface/hub/ReaderLM-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Patterns\n",
    "SCRIPT_PATTERN = r\"<[ ]*script.*?\\/[ ]*script[ ]*>\"\n",
    "STYLE_PATTERN = r\"<[ ]*style.*?\\/[ ]*style[ ]*>\"\n",
    "META_PATTERN = r\"<[ ]*meta.*?>\"\n",
    "COMMENT_PATTERN = r\"<[ ]*!--.*?--[ ]*>\"\n",
    "LINK_PATTERN = r\"<[ ]*link.*?>\"\n",
    "BASE64_IMG_PATTERN = r'<img[^>]+src=\"data:image/[^;]+;base64,[^\"]+\"[^>]*>'\n",
    "SVG_PATTERN = r\"(<svg[^>]*>)(.*?)(<\\/svg>)\"\n",
    "\n",
    "\n",
    "def replace_svg(html: str, new_content: str = \"this is a placeholder\") -> str:\n",
    "    return re.sub(\n",
    "        SVG_PATTERN,\n",
    "        lambda match: f\"{match.group(1)}{new_content}{match.group(3)}\",\n",
    "        html,\n",
    "        flags=re.DOTALL,\n",
    "    )\n",
    "\n",
    "\n",
    "def replace_base64_images(html: str, new_image_src: str = \"#\") -> str:\n",
    "    return re.sub(BASE64_IMG_PATTERN, f'<img src=\"{new_image_src}\"/>', html)\n",
    "\n",
    "\n",
    "def clean_html(html: str, clean_svg: bool = True, clean_base64: bool = True):\n",
    "    html = re.sub(\n",
    "        SCRIPT_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "    html = re.sub(\n",
    "        STYLE_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "    html = re.sub(\n",
    "        META_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "    html = re.sub(\n",
    "        COMMENT_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "    html = re.sub(\n",
    "        LINK_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    if clean_svg:\n",
    "        html = replace_svg(html)\n",
    "    if clean_base64:\n",
    "        html = replace_base64_images(html)\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF.ReaderLM-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "def html_to_markdown(html_text: str) -> str:\n",
    "    # instruction = \"Extract the specified information from a list of news threads and present it in a structured JSON format.\"\n",
    "    # prompt = f\"{instruction}\\n```html\\n{html_text}\\n```\\nThe JSON schema is as follows:```json\\n{schema}\\n```\"\n",
    "    instruction = \"Extract the main content from the given HTML and convert it to Markdown format.\"\n",
    "    prompt = f\"{instruction}\\n```html\\n{html_text}\\n```\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"jinaai/ReaderLM-v2:featherless-ai\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "\n",
    "with open('temp/input.html', 'r') as f:\n",
    "    html_text = f.read()\n",
    "    html_text2 = clean_html(html_text)\n",
    "    print(len(html_text), len(html_text2))\n",
    "\n",
    "md = html_to_markdown(html_text2)\n",
    "\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://r.jina.ai/https://news.ycombinator.com/ \\\n",
    "    -H 'x-engine: readerlm-v2' \\\n",
    "    -H 'Accept: text/event-stream' \\\n",
    "    -H \"Authorization: Bearer jina_713bf0edf5524b5fa413400f059f168cUlcdiQY0HPE8xfixKtjex7-Jracj\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"  # or \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/ReaderLM-v2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"jinaai/ReaderLM-v2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    html_text: str, tokenizer=None, instruction: str = None, schema: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for the model with optional instruction and JSON schema.\n",
    "    \"\"\"\n",
    "    html_text = clean_html(html_text)\n",
    "    if not instruction:\n",
    "        instruction = \"Extract the main content from the given HTML and convert it to Markdown format.\"\n",
    "    if schema:\n",
    "        instruction = \"Extract the specified information from a list of news threads and present it in a structured JSON format.\"\n",
    "        prompt = f\"{instruction}\\n```html\\n{html_text}\\n```\\nThe JSON schema is as follows:```json\\n{schema}\\n```\"\n",
    "    else:\n",
    "        prompt = f\"{instruction}\\n```html\\n{html_text}\\n```\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"<html><body><h1>Hello, world!</h1></body></html>\"\n",
    "html = Path('temp/input.html').read_text()\n",
    "# memory issue for long context.\n",
    "\n",
    "input_prompt = create_prompt(html, tokenizer=tokenizer)\n",
    "inputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
