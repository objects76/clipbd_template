[Sitemap](/sitemap/sitemap.xml)[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3be6ea745f0f&%7Efeature=LiOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)Sidebar menu

[Write](https://medium.com/new-story?source=post_page---top_nav_layout_nav-----------------------------------------)[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)[Notifications](/me/notifications?source=post_page---top_nav_layout_nav-----------------------------------------)![Ai_rsupport](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)

Sidebar menu

[Home](/?source=post_page---sidebar_menu-----------------------------------------)[Library](/me/lists?source=post_page---sidebar_menu-----------------------------------------)[Profile](/@ai_17047?source=post_page---sidebar_menu-----------------------------------------)[Stories](/me/stories?source=post_page---sidebar_menu-----------------------------------------)[Stats](/me/stats?source=post_page---sidebar_menu-----------------------------------------)[Following](/me/following-feed/writers?source=post_page---sidebar_menu-----------------------------------------)[![Joe Njenga](https://miro.medium.com/v2/resize:fill:32:32/1*0Hoc7r7_ybnOvk1t8yR3_A.jpeg)Joe Njenga](/me/following-feed/writers/15bfc6bce2c9?source=post_page---sidebar_menu_following------------------------------following_badge-----------)[![Liu Zuo Lin](https://miro.medium.com/v2/resize:fill:32:32/1*Z5dMY4-vS6G69lMMdn3xIQ.jpeg)Liu Zuo Lin](/me/following-feed/writers/e5f30f9ccdaa?source=post_page---sidebar_menu_following------------------------------following_badge-----------)[![Folarin Favour Olaoluwapo](https://miro.medium.com/v2/resize:fill:32:32/1*E0HvKMHFu77zIoog_pwnDw.jpeg)Folarin Favour Olaoluwapo](/me/following-feed/writers/5ce89a4a0fc0?source=post_page---sidebar_menu_following-----------------------------------------)[![Fareed Khan](https://miro.medium.com/v2/resize:fill:32:32/1*feiUXOR8sid6IPHSHufA-g.jpeg)Fareed Khan](/me/following-feed/writers/b856005e5ecd?source=post_page---sidebar_menu_following-----------------------------------------)[![Jordan Gibbs](https://miro.medium.com/v2/resize:fill:32:32/1*A1IH-oySSaC8eRBAxDhWxQ.jpeg)Jordan Gibbs](/me/following-feed/writers/4beacba7dc8a?source=post_page---sidebar_menu_following------------------------------following_badge-----------)[![Fabio Matricardi](https://miro.medium.com/v2/resize:fill:32:32/1*p4ShYlP7zymOUeIZ5DSbfg.png)Fabio Matricardi](/me/following-feed/writers/62d264ed2dcb?source=post_page---sidebar_menu_following------------------------------following_badge-----------)[![Nikhil Anand](https://miro.medium.com/v2/resize:fill:32:32/1*WUi3_JE6nFyvnhbWnBxYKg.jpeg)Nikhil Anand](/me/following-feed/writers/a08d6bc7c07?source=post_page---sidebar_menu_following------------------------------following_badge-----------)[![Piyush Agnihotri](https://miro.medium.com/v2/resize:fill:32:32/1*pKV1QsobYv5WWiTJwAF71Q.jpeg)Piyush Agnihotri](/me/following-feed/writers/953f3bab3dce?source=post_page---sidebar_menu_following------------------------------following_badge-----------)[![N8N Instances](https://miro.medium.com/v2/resize:fill:32:32/1*OtCEt1VX5_hMYdE_L6PKqA.jpeg)N8N Instances](/me/following-feed/publications/381bac2a57d6?source=post_page---sidebar_menu_following-----------------------------------------)[![KAD Blogging](https://miro.medium.com/v2/resize:fill:32:32/1*TQwDmo8ZzbZdGaZrV9bBLg.jpeg)KAD Blogging](/me/following-feed/writers/378f519d2449?source=post_page---sidebar_menu_following------------------------------following_badge-----------)Find writers and publications to follow.

[See suggestions](/me/following/suggestions?source=post_page---sidebar_menu_following-----------------------------------------)[Data Science Collective
-----------------------](https://medium.com/data-science-collective?source=post_page---publication_nav-8993e01dcfd3-3be6ea745f0f---------------------------------------)·Follow publication

[![Data Science Collective](https://miro.medium.com/v2/resize:fill:76:76/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---post_publication_sidebar-8993e01dcfd3-3be6ea745f0f---------------------------------------)Advice, insights, and ideas from the Medium data science community

Follow publication

Unleashing LLMs with Team Red: Your Guide to AMD-Powered Local AI
=================================================================

Because your gaming GPU can do more than just crush frames
----------------------------------------------------------

[![Vikas Negi](https://miro.medium.com/v2/resize:fill:64:64/2*1xvONKOlT77RqFmciJS_UQ.jpeg)](/@vikas.negi10?source=post_page---byline--3be6ea745f0f---------------------------------------)[Vikas Negi](/@vikas.negi10?source=post_page---byline--3be6ea745f0f---------------------------------------)Follow

10 min read·Apr 30, 2025142

3

Listen

Share

More

![](https://miro.medium.com/v2/resize:fit:1400/1*4VzWqCm0dx_1dtrPTF-IYw.png)

*Gigabyte RX 7900 XT (Ghibli art created by ChatGPT 4o- Image uploaded by author)*

When it comes to AI and machine learning, one company has undoubtedly dominated the conversation: **NVIDIA**. Their CUDA compute stack and powerful GPUs have become the default choice for training and running large language models (LLMs). From casual experiments to cutting-edge research, it often feels like if you don’t have a shiny green GPU, you’re simply left out of the game. But what about their closest competitor **Team Red**?

AMD (aka Team Red) has been around for a long time, both in the CPU and GPU business. On the CPU side, they have gone head to head with Intel, even [overtaking them in data center sales](https://www.tomshardware.com/pc-components/cpus/amd-outsells-intel-in-the-datacenter-for-the-first-time-in-q4-2024) quite recently. However, on the GPU side competing against Nvidia, they have largely shifted their focus on the mid-range gaming market. AMD’s offering includes powerful GPUs at competitive prices — yet when it comes to AI, particularly LLM inferencing, clear and reliable guides for AMD users are frustratingly hard to find.

I recently upgraded my personal gaming rig and replaced my old (but gold!) GTX 1080Ti with the Radeon RX 7900XT. It has 5376 stream processors (conceptually similar but not one-to-one comparable to CUDA cores) and 20 GB of GDDR6 memory over a 320 bit bus width. When not playing games, I figured that it would be great to leverage this beast to run a local LLM assistant for my data science projects. To get there, I spent a lot of time troubleshooting issues with ROCm installation on Linux. I have summarized the most important findings in this article.

![](https://miro.medium.com/v2/resize:fit:1400/1*lLdf2nZnfZb0Phf7vpQxng.jpeg)

*Leaving the old guard behind. Left: Nvidia GeForce GTX 1080Ti, Right: AMD Radeon RX 7900XT (Image by author).*

Setup and configuration
-----------------------

Our LLM tech stack will consist of ROCm + Ollama + Open WebUI. It’s important to note that some of the steps in this guide will depend on the target platform characteristics such as the operating system. We will focus only on Linux. I use Elementary OS as my distro of choice, but the guide will be valid for most Ubuntu/Debian based systems. For the rest, you will need to find the equivalent terminal commands.

1. Install GPU drivers

This is the first step that needs to be done after plugging in a new GPU. Apart from the video drivers, we want to install Radeon Open Compute Platform (ROCm), which is what Ollama will make use of when running inferencing tasks on the GPU. The Nvidia equivalent is the well-known CUDA platform.

* First, check this [link](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility/native_linux/native_linux_compatibility.html) to understand which versions of ROCm are compatible with a given GPU and Linux kernel. Check your kernel version:

```
$ uname -r  
6.8.0-40-generic
```
* For my kernel version, ROCm version 6.2.x is the best bet as shown [here](https://rocm.docs.amd.com/projects/radeon/en/docs-6.2/docs/compatibility/native_linux/native_linux_compatibility.html). I tried a few versions, and 6.2.1 worked out to be the most suitable.
* Install the `amdgpu-install` utility from [here](https://repo.radeon.com/amdgpu-install/6.2.1/ubuntu/jammy/), for example:

```
wget https://repo.radeon.com/amdgpu-install/6.2.1/ubuntu/jammy/amdgpu-install_6.2.60201-1_all.deb  
sudo apt-get install ./amdgpu-install_6.1.60103-1_all.deb 
```
* Run the install script with the following options. It will take a while to complete.

```
sudo amdgpu-install --usecase=dkms,graphics,multimedia,rocm,rocmdev,opencl,openclsdk,hip,hiplibsdk
```
* Pay attention to any errors that appear and google for fixes accordingly. For instance, I had some issue with an incompatible gcc version, and had to switch to version 12. This error usually appears as *“gcc version does not match version that built latest default kernel”.*

```
$ gcc -v  
Using built-in specs.  
COLLECT_GCC=gcc  
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/12/lto-wrapper  
OFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa  
OFFLOAD_TARGET_DEFAULT=1  
Target: x86_64-linux-gnu  
...  
Thread model: posix  
Supported LTO compression algorithms: zlib zstd  
gcc version 12.3.0 (Ubuntu 12.3.0-1ubuntu1~22.04)
```
2. Check drivers

Once the drivers and ROCm have been installed successfully, we need to reboot the system and run some basic checks as shown below with sample output for my setup:

* Current display driver (pay attention to the line that says *driver=amdgpu*)

```
$ sudo lshw -C display  
  
  *-display                   
       description: VGA compatible controller  
       product: Navi 31 [Radeon RX 7900 XT/7900 XTX/7900 GRE/7900M]  
       vendor: Advanced Micro Devices, Inc. [AMD/ATI]  
       physical id: 0  
       bus info: pci@0000:08:00.0  
       logical name: /dev/fb0  
       version: cc  
       width: 64 bits  
       clock: 33MHz  
       capabilities: pm pciexpress msi vga_controller bus_master cap_list rom fb  
       configuration: depth=32 driver=amdgpu latency=0 resolution=3440,1440  
       resources: irq:102 memory:d0000000-dfffffff memory:e0000000-e01fffff ioport:e000(size=256) memory:fc900000-fc9fffff memory:c0000-dffff
```
* Kernel driver is installed okay

```
$ dkms status  
  
amdgpu/6.8.5-2038383.22.04, 6.8.0-40-generic, x86_64: installed (original_module exists)  
amdgpu/6.8.5-2038383.22.04, 6.8.0-52-generic, x86_64: installed (original_module exists)
```
* GPU listing as an agent (= 2) is okay. Agent 1 will be the CPU.

```
$ rocminfo  
   
ROCk module version 6.8.5 is loaded  
=====================      
HSA System Attributes      
=====================      
Runtime Version:         1.14  
Runtime Ext Version:     1.6  
System Timestamp Freq.:  1000.000000MHz  
Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)  
Machine Model:           LARGE                                
System Endianness:       LITTLE                               
Mwaitx:                  DISABLED  
DMAbuf Support:          YES  
...  
******* 
Agent 2                    
******* 
  Name:                    gfx1100                              
  Uuid:                    GPU-c6f8040818892811                 
  Marketing Name:          Radeon RX 7900 XT                    
  Vendor Name:             AMD                                  
  Feature:                 KERNEL_DISPATCH                      
  Profile:                 BASE_PROFILE                         
  Float Round Mode:        NEAR                                 
  Max Queue Number:        128(0x80)                            
  Queue Min Size:          64(0x40)                             
  Queue Max Size:          131072(0x20000)                      
  Queue Type:              MULTI                                
  Node:                    1                                    
  Device Type:             GPU                               
...
```
* GPU platform is listed correctly

```
$ clinfo   
  
Number of platforms:     1  
  Platform Profile:     FULL_PROFILE  
  Platform Version:     OpenCL 2.1 AMD-APP (3625.0)  
  Platform Name:     AMD Accelerated Parallel Processing  
  Platform Vendor:     Advanced Micro Devices, Inc.  
  Platform Extensions:     cl_khr_icd cl_amd_event_callback   
  
  
  Platform Name:     AMD Accelerated Parallel Processing  
Number of devices:     1  
  Device Type:      CL_DEVICE_TYPE_GPU  
  Vendor ID:      1002h  
  Board name:      Radeon RX 7900 XT  
  Device Topology:     PCI[ B#8, D#0, F#0 ]  
  Max compute units:     42  
  Max work items dimensions:    3  
    Max work items[0]:     1024  
    Max work items[1]:     1024  
    Max work items[2]:     1024  
  Max work group size:     256  
...
```
3. GPU monitoring tool

It is both informative and important to monitor the resource usage of our GPU when running the LLM. This will help us optimize for performance and efficiency later on. For instance, depending on the load or power consumption, we can choose to offload more or less layers from a model towards the GPU. On Linux, the `[amdgpu_top](https://github.com/Umio-Yasuno/amdgpu_top)` is a fantastic Rust-based tool to monitor metrics for AMD GPUs. You can easily install it via `cargo`.

I prefer to run it in the GUI mode: `amdgpu_top --gui`

![](https://miro.medium.com/v2/resize:fit:1400/1*OrQmiW4Pd0DLoGyotvh44g.png)

*amdgpu_top in action. Device info is available on the left, while the metrics are continuously updated on the right. (Image by author)*

![](https://miro.medium.com/v2/resize:fit:1400/1*Qti6NWvB7-9G__dNebHkYA.gif)

*GPU under full load, when LLM is generating a response. Power consumption peaks at ~ 281 W. (Image GIF by author)*

4. Install Ollama and Open WebUI

* Installing [Ollama](https://ollama.com/download) is pretty straightforward these days. I won’t go into the details here. What is important is that Ollama detects our AMD ROCm stack and downloads the appropriate version. Sample output is shown below:

```
$ curl -fsSL https://ollama.com/install.sh | sh  
  
>>> Cleaning up old version at /usr/local/lib/ollama  
[sudo] password for vikas:               
>>> Installing ollama to /usr/local  
>>> Downloading Linux amd64 bundle  
######################################################################## 100.0%  
>>> Adding ollama user to render group...  
>>> Adding ollama user to video group...  
>>> Adding current user to ollama group...  
>>> Creating ollama systemd service...  
>>> Enabling and starting ollama service...  
>>> Downloading Linux ROCm amd64 bundle  
######################################################################## 100.0%  
>>> The Ollama API is now available at 127.0.0.1:11434.  
>>> Install complete. Run "ollama" from the command line.  
>>> AMD GPU ready.
```
* Check if Ollama background service has started running. This is important for connecting to Open WebUI.

```
$ sudo systemctl status ollama.service   
● ollama.service - Ollama Service  
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled; vendor preset: enabled)  
    Drop-In: /etc/systemd/system/ollama.service.d  
             └─override.conf  
     Active: active (running) since Sat 2025-04-05 08:45:05 CEST; 1 day 7h ago  
   Main PID: 2857 (ollama)  
      Tasks: 29 (limit: 76822)  
     Memory: 12.3M  
        CPU: 2h 19min 29.139s  
     CGroup: /system.slice/ollama.service  
             └─2857 /usr/local/bin/ollama serve
```
The default API URL for Ollama is http://127.0.0.1:11434. This is used as an input to the Docker command below.

* Open WebUI is a popular solution for the LLM frontend. It is quite easy to set up and run as a Docker container. Make sure Docker is set up correctly before running the following commands.

```
docker pull ghcr.io/open-webui/open-webui:main
```

```
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
Once the Docker container is up and running, you can access the Open WebUI interface at <http://localhost:8080>. This need not be on your own computer. Within the same home network, the web UI can also be accessed via another device by visiting the URL: <host-ip-address>:8080

Benchmarks
----------

Now that we are all set, it’s time to put the GPU through its paces by running some benchmarks. We can make use of the `[ollama-benchmark](https://github.com/aidatatools/ollama-benchmark)` Python package to achieve this. Note that it needs a working ollama installation, which we already have. The benchmarks can be triggered for multiple models by creating a model input file, for example:

```
$ cat deepseek_r1_models.yaml   
  
file_name: "deepseek_r1_models.yaml"  
version: 1.0  
models:  
  - model: "deepseek-r1:8b"  
  - model: "deepseek-r1:14b"  
  - model: "deepseek-r1:32b"
```
Using [poetry](https://python-poetry.org/docs/basic-usage/#using-your-virtual-environment) as the package manager, the benchmarks can be started via:

```
# cd to the git repository  
  
poetry install  
  
poetry env activate  
  
poetry run llm_benchmark run --custombenchmark=deepseek_r1_models.yaml
```
These models will first be downloaded, and then the benchmark run will cycle through them. The benchmark itself consists of a series of prompts, and returns the average tokens/sec at the end.

```
-------Linux----------  
  
No GPU detected.  
Total memory size : 31.24 GB  
cpu_info: AMD Ryzen 9 5900X 12-Core Processor  
gpu_info: Navi 31 [Radeon RX 7900 XT/7900 XTX/7900 GRE/7900M]  
os_version: elementary OS 7.1 Horus  
ollama_version: 0.5.12  
----------  
running custom benchmark from models_file_path: deepseek_r1_models.yaml  
Disabling sendinfo for custom benchmark  
LLM models file path：deepseek_r1_models.yaml  
Checking and pulling the following LLM models  
deepseek-r1:8b  
deepseek-r1:14b  
deepseek-r1:32b  
----------  
Running custom-model  
model_name =    deepseek-r1:8b  
prompt = Summarize the key differences between classical and operant conditioning in psychology.  
eval rate:            73.81 tokens/s  
prompt = Translate the following English paragraph into Chinese and elaborate more -> Artificial intelligence is transforming various industries by enhancing efficiency and enabling new capabilities.  
eval rate:            74.27 tokens/s  
prompt = What are the main causes of the American Civil War?  
eval rate:            73.83 tokens/s  
prompt = How does photosynthesis contribute to the carbon cycle?  
eval rate:            74.27 tokens/s  
prompt = Develop a python function that solves the following problem, sudoku game.  
eval rate:            72.41 tokens/s  
--------------------  
Average of eval rate:  73.718  tokens/s  
----------------------------------------  
  
model_name =    deepseek-r1:14b  
prompt = Summarize the key differences between classical and operant conditioning in psychology.  
eval rate:            43.12 tokens/s  
prompt = Translate the following English paragraph into Chinese and elaborate more -> Artificial intelligence is transforming various industries by enhancing efficiency and enabling new capabilities.  
eval rate:            44.50 tokens/s  
prompt = What are the main causes of the American Civil War?  
eval rate:            43.86 tokens/s  
prompt = How does photosynthesis contribute to the carbon cycle?  
eval rate:            42.99 tokens/s  
prompt = Develop a python function that solves the following problem, sudoku game.  
eval rate:            40.75 tokens/s  
--------------------  
Average of eval rate:  43.044  tokens/s  
----------------------------------------  
  
model_name =    deepseek-r1:32b  
prompt = Summarize the key differences between classical and operant conditioning in psychology.  
eval rate:            9.95 tokens/s  
prompt = Translate the following English paragraph into Chinese and elaborate more -> Artificial intelligence is transforming various industries by enhancing efficiency and enabling new capabilities.  
eval rate:            9.91 tokens/s  
prompt = What are the main causes of the American Civil War?  
eval rate:            9.91 tokens/s  
prompt = How does photosynthesis contribute to the carbon cycle?  
eval rate:            9.92 tokens/s  
prompt = Develop a python function that solves the following problem, sudoku game.  
eval rate:            9.39 tokens/s  
--------------------  
Average of eval rate:  9.816  tokens/s  
----------------------------------------
```
As expected, the performance goes down as the model size increases. It’s not always necessary to make use of a model with the biggest parameter set. Ultimately, it’s a tradeoff between performance and accuracy of the results. Additional benchmarks for other models with different quantization can be found in this [GitHub Gist](https://gist.github.com/vnegi10/bbc10a177d529644ba13614ad52c1fe0). If you are curious about performance on NVIDIA hardware, check out the results for Linux [here](https://llm.aidatatools.com/results-linux.php).

The RTX 5090 leads the charts with an impressive ~ 122 tokens/sec for deepseek-r1:14b. This is much higher than what our 700 EUR AMD card could manage at 43 tokens/sec. However, do keep in mind that the 5090 is a much more expensive high-end card (pricing is all over the place with listings above 2200 EUR) with [potential risks](https://www.tomshardware.com/pc-components/gpus/another-rtx-5090-connector-melts-down-reportedly-taking-a-redditors-psu-with-it) of burning down your house. I personally find that anything above 30 tokens/sec is very usable. That coupled with the affordability, safety and availability of our AMD GPU is no less than a win!

User experience
---------------

Here’s how the Open WebUI interface feels and looks like. I am using the excellent *qwen2.5-coder:14b* model as a Data Science assistant. The response throughput is ~ 38 tokens/sec.

![](https://miro.medium.com/v2/resize:fit:1400/1*G2b5VKlfCOo0ikw7uVKRgA.gif)

*Our Local LLM in action shown in real time(Image GIF by author)*

Additional settings
-------------------

* Models can easily be searched for and downloaded when not available

![](https://miro.medium.com/v2/resize:fit:1400/1*-Cb5Mv7Yaj5Y-sIRN-tXhg.gif)

*Downloading a new model (Image GIF by author)*

* A customized system prompt can be provided by opening the **Chat Controls** setting from top right

![](https://miro.medium.com/v2/resize:fit:1400/1*D0Gu3H7kAkQXZk9cCl5mvA.png)

*Example system prompt (Image by author)*

* To change the number of layers offloaded to GPU, check the `num_gpu` setting at the bottom in the same settings.

![](https://miro.medium.com/v2/resize:fit:1400/1*kwP7DRA131VFmp-EVnJeQQ.png)

*Additional performance settings (Image by author)*

* When `num_gpu` is set to 0, inferencing will be done on the CPU. While this can be more efficient in terms of power consumption, the performance will be subpar (12 tokens/sec for deepseek-coder-v2) compared to the GPU (60 tokens/sec).

![](https://miro.medium.com/v2/resize:fit:1400/1*l8NVCuZEmNpR0F2OtDUHrQ.png)

*CPU being hammered during inferencing (Image by author)*

Conclusion
----------

Setting up LLM inferencing on AMD hardware isn’t always as plug-and-play as it is with CUDA — yet. But with the right tools, some patience, and an unwavering spirit, it’s absolutely possible to turn your gaming rig into a powerful AI machine. And the best part? You’re doing it the open-source way, helping diversify the AI hardware ecosystem.

The future is of course bigger than just green vs. red — it’s about giving more people the power to create. In a world where AI is shaping everything, the ability to run your own models locally is a kind of new-found freedom. With affordable mid-range GPU’s, that freedom is now within reach for us common folks as well.

I hope you found this guide useful. Thank you for your time! Connect with me on [LinkedIn](https://www.linkedin.com/in/negivikas/) or visit my [Web 3.0 powered website](https://vikasnegi.eth.limo/).

References
----------

1. <https://rocm.docs.amd.com/en/latest/>
2. <https://github.com/ollama/ollama>
3. <https://docs.openwebui.com/>

[AI](/tag/ai?source=post_page-----3be6ea745f0f---------------------------------------)[Gpu](/tag/gpu?source=post_page-----3be6ea745f0f---------------------------------------)[Llm](/tag/llm?source=post_page-----3be6ea745f0f---------------------------------------)[Ollama](/tag/ollama?source=post_page-----3be6ea745f0f---------------------------------------)[Amd](/tag/amd?source=post_page-----3be6ea745f0f---------------------------------------)142

142

3

[![Data Science Collective](https://miro.medium.com/v2/resize:fill:96:96/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---post_publication_info--3be6ea745f0f---------------------------------------)[![Data Science Collective](https://miro.medium.com/v2/resize:fill:128:128/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---post_publication_info--3be6ea745f0f---------------------------------------)Follow

[Published in Data Science Collective
------------------------------------](https://medium.com/data-science-collective?source=post_page---post_publication_info--3be6ea745f0f---------------------------------------)[872K followers](/data-science-collective/followers?source=post_page---post_publication_info--3be6ea745f0f---------------------------------------)·[Last published just now](/data-science-collective/5-ai-projects-youll-actually-want-to-try-to-sharpen-your-skills-285f05106ff2?source=post_page---post_publication_info--3be6ea745f0f---------------------------------------)Advice, insights, and ideas from the Medium data science community

Follow

[![Vikas Negi](https://miro.medium.com/v2/resize:fill:96:96/2*1xvONKOlT77RqFmciJS_UQ.jpeg)](/@vikas.negi10?source=post_page---post_author_info--3be6ea745f0f---------------------------------------)[![Vikas Negi](https://miro.medium.com/v2/resize:fill:128:128/2*1xvONKOlT77RqFmciJS_UQ.jpeg)](/@vikas.negi10?source=post_page---post_author_info--3be6ea745f0f---------------------------------------)Follow

[Written by Vikas Negi
---------------------](/@vikas.negi10?source=post_page---post_author_info--3be6ea745f0f---------------------------------------)[178 followers](/@vikas.negi10/followers?source=post_page---post_author_info--3be6ea745f0f---------------------------------------)·[176 following](/@vikas.negi10/following?source=post_page---post_author_info--3be6ea745f0f---------------------------------------)Senior Data Scientist at AkzoNobel • PhD in Applied Physics

Follow

Responses (3)
-------------

![Ai_rsupport](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Ai_rsupportCancel

Respond

[![Daddi Ananta](https://miro.medium.com/v2/resize:fill:64:64/1*pzM81Z4nduYeADGgXh_iGw.png)](/@daddyananta?source=post_page---post_responses--3be6ea745f0f----0-----------------------------------)[Daddi Ananta](/@daddyananta?source=post_page---post_responses--3be6ea745f0f----0-----------------------------------)[May 2](/@daddyananta/excellent-post-thank-you-for-sharing-0af0704eb19d?source=post_page---post_responses--3be6ea745f0f----0-----------------------------------)
```
Excellent post, thank you for sharing!
```
11

1 reply

Reply

[![Gulshan N](https://miro.medium.com/v2/resize:fill:64:64/1*eNoWot5zhAA0Pm1hi3IK-Q.jpeg)](/@gulshan_n?source=post_page---post_responses--3be6ea745f0f----1-----------------------------------)[Gulshan N](/@gulshan_n?source=post_page---post_responses--3be6ea745f0f----1-----------------------------------)[May 2](/@gulshan_n/amazing-insights-b13244aea520?source=post_page---post_responses--3be6ea745f0f----1-----------------------------------)
```
Amazing insights! It's exciting to see how AMD GPUs can power local AI tasks, not just gaming. Thanks for highlighting the broader potential.
```
10

1 reply

Reply

[![Alec Novella](https://miro.medium.com/v2/resize:fill:64:64/1*MAREJ-l17UsIy3lEPpp_Dw.png)](/@novellaalec?source=post_page---post_responses--3be6ea745f0f----2-----------------------------------)[Alec Novella](/@novellaalec?source=post_page---post_responses--3be6ea745f0f----2-----------------------------------)[May 10](/@novellaalec/really-great-article-gonna-put-some-of-my-farming-gpus-to-work-1cd3ca8b63f8?source=post_page---post_responses--3be6ea745f0f----2-----------------------------------)
```
really great article! gonna put some of my farming GPUs to work
```
Reply

More from Vikas Negi and Data Science Collective
------------------------------------------------

![A Simple Trick to Speed Up Reading CSV Files in Julia](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*eVZ26hNMONNzYS6eN-_PaA.png)[![Vikas Negi](https://miro.medium.com/v2/resize:fill:40:40/2*1xvONKOlT77RqFmciJS_UQ.jpeg)](/@vikas.negi10?source=post_page---author_recirc--3be6ea745f0f----0---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)[Vikas Negi](/@vikas.negi10?source=post_page---author_recirc--3be6ea745f0f----0---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)[A Simple Trick to Speed Up Reading CSV Files in Julia
-----------------------------------------------------

### Let’s learn how to accelerate your data science workflow](/@vikas.negi10/a-simple-trick-to-speed-up-reading-csv-files-in-julia-749856a1ca4?source=post_page---author_recirc--3be6ea745f0f----0---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)Jul 9, 2023[10](/@vikas.negi10/a-simple-trick-to-speed-up-reading-csv-files-in-julia-749856a1ca4?source=post_page---author_recirc--3be6ea745f0f----0---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)

![Have You Been Workslopped Yet?](https://miro.medium.com/v2/resize:fit:1358/format:webp/0*aBQsEwU_P366pm3j.png)[![Data Science Collective](https://miro.medium.com/v2/resize:fill:40:40/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---author_recirc--3be6ea745f0f----1---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)In

[Data Science Collective](https://medium.com/data-science-collective?source=post_page---author_recirc--3be6ea745f0f----1---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)by

[Cassie Kozyrkov](/@kozyrkov?source=post_page---author_recirc--3be6ea745f0f----1---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)[Have You Been Workslopped Yet?
------------------------------

### AI is making you clean up your colleague’s mess. Here’s what should you do about it.](/data-science-collective/have-you-been-workslopped-yet-d175a8e1a68f?source=post_page---author_recirc--3be6ea745f0f----1---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)Oct 4[91723](/data-science-collective/have-you-been-workslopped-yet-d175a8e1a68f?source=post_page---author_recirc--3be6ea745f0f----1---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)

![The Starters Guide to Causal Structure Learning with Bayesian Methods in Python.](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*XjVrNv7NjZuEdXdAwtZ0lA.jpeg)[![Data Science Collective](https://miro.medium.com/v2/resize:fill:40:40/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---author_recirc--3be6ea745f0f----2---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)In

[Data Science Collective](https://medium.com/data-science-collective?source=post_page---author_recirc--3be6ea745f0f----2---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)by

[Erdogan T](/@erdogant?source=post_page---author_recirc--3be6ea745f0f----2---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)[The Starters Guide to Causal Structure Learning with Bayesian Methods in Python.
--------------------------------------------------------------------------------

### The starter’s guide to effectively learn to determine causalities across variables.](/data-science-collective/the-starters-guide-to-causal-structure-learning-with-bayesian-methods-in-python-e3b90f49c99c?source=post_page---author_recirc--3be6ea745f0f----2---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)Sep 19[4146](/data-science-collective/the-starters-guide-to-causal-structure-learning-with-bayesian-methods-in-python-e3b90f49c99c?source=post_page---author_recirc--3be6ea745f0f----2---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)

![Predicting Gas Consumption using Machine Learning in Julia](https://miro.medium.com/v2/resize:fit:1358/format:webp/0*T4aGNfahuhLaMlCg)[![Vikas Negi](https://miro.medium.com/v2/resize:fill:40:40/2*1xvONKOlT77RqFmciJS_UQ.jpeg)](/@vikas.negi10?source=post_page---author_recirc--3be6ea745f0f----3---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)[Vikas Negi](/@vikas.negi10?source=post_page---author_recirc--3be6ea745f0f----3---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)[Predicting Gas Consumption using Machine Learning in Julia
----------------------------------------------------------

### A data-driven approach to combat the energy crisis](/@vikas.negi10/predicting-gas-consumption-using-machine-learning-in-julia-92a0441cc8cf?source=post_page---author_recirc--3be6ea745f0f----3---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)Aug 26, 2023[29](/@vikas.negi10/predicting-gas-consumption-using-machine-learning-in-julia-92a0441cc8cf?source=post_page---author_recirc--3be6ea745f0f----3---------------------7d73a696_d984_4bbe_86cc_10fddf0e8af9--------------)

[See all from Vikas Negi](/@vikas.negi10?source=post_page---author_recirc--3be6ea745f0f---------------------------------------)[See all from Data Science Collective](https://medium.com/data-science-collective?source=post_page---author_recirc--3be6ea745f0f---------------------------------------)

Recommended from Medium
-----------------------

![Building Long-Term Memory in Agentic AI](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*W9xFA-p9sulzFjGi4Uodlw.png)[![Level Up Coding](https://miro.medium.com/v2/resize:fill:40:40/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg)](https://medium.com/gitconnected?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)In

[Level Up Coding](https://medium.com/gitconnected?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)by

[Fareed Khan](/@fareedkhandev?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Building Long-Term Memory in Agentic AI
---------------------------------------

### HITL, InMemory Feature, Feedback Loop and more](/gitconnected/building-long-term-memory-in-agentic-ai-2941b0cca3bf?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)5d ago[6409](/gitconnected/building-long-term-memory-in-agentic-ai-2941b0cca3bf?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)

![Using AI (LLM) with Obsidian Notes](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*j9LC4ex5We6R04Ixi1-Oow.png)[![Mayeenul Islam](https://miro.medium.com/v2/resize:fill:40:40/1*LmZVA_IGxHz3WDDo9zb9KA.jpeg)](/@mayeenulislam?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Mayeenul Islam](/@mayeenulislam?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Using AI (LLM) with Obsidian Notes
----------------------------------

### There are plenty of other ways you could use LLM with Obsidian Notes, but here are my tested approaches.](/@mayeenulislam/using-ai-llm-with-obsidian-notes-82a545eba7b3?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)Oct 7

![No Libraries, No Shortcuts: LLM from Scratch with PyTorch](https://miro.medium.com/v2/resize:fit:1358/format:webp/0*1Q_Tx43V1NBH2U_3.png)[![Towards AI](https://miro.medium.com/v2/resize:fill:40:40/1*JyIThO-cLjlChQLb6kSlVQ.png)](https://medium.com/towards-artificial-intelligence?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)In

[Towards AI](https://medium.com/towards-artificial-intelligence?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)by

[Ashish Abraham](/@ashishabraham02?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[No Libraries, No Shortcuts: LLM from Scratch with PyTorch
---------------------------------------------------------

### The no BS guide to build, train, and fine-tune a Transformer architecture from scratch](/towards-artificial-intelligence/no-libraries-no-shortcuts-llm-from-scratch-with-pytorch-664c557997ee?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)Oct 2[1K15](/towards-artificial-intelligence/no-libraries-no-shortcuts-llm-from-scratch-with-pytorch-664c557997ee?source=post_page---read_next_recirc--3be6ea745f0f----0---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)

![AI Just Had Its “Big Short” Moment](https://miro.medium.com/v2/resize:fit:1358/format:webp/0*0jzGy3_PK-sokoo6)[![Will Lockett](https://miro.medium.com/v2/resize:fill:40:40/1*V0qWMQ8V5_NaF9yUoHAdyg.jpeg)](/@wlockett?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Will Lockett](/@wlockett?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[AI Just Had Its “Big Short” Moment
----------------------------------

### The grfit is up.](/@wlockett/ai-just-had-its-big-short-moment-714e143dabfa?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)3d ago[3.7K100](/@wlockett/ai-just-had-its-big-short-moment-714e143dabfa?source=post_page---read_next_recirc--3be6ea745f0f----1---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)

![Stop Using the Default Terminal — Here’s the Setup You Deserve](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*0AZTpO2eJoTXU5GBeZq-8A.png)[![Ashish Singh](https://miro.medium.com/v2/resize:fill:40:40/1*LwwW8iuLsLBasPSo9kaZPw.jpeg)](/@ashishnoob?source=post_page---read_next_recirc--3be6ea745f0f----2---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Ashish Singh](/@ashishnoob?source=post_page---read_next_recirc--3be6ea745f0f----2---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Stop Using the Default Terminal — Here’s the Setup You Deserve
--------------------------------------------------------------

### Transform your dull command line into a productivity powerhouse that looks amazing and works even better.](/@ashishnoob/stop-using-the-default-terminal-heres-the-setup-you-deserve-bf5e52b51e5d?source=post_page---read_next_recirc--3be6ea745f0f----2---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)Oct 8[933](/@ashishnoob/stop-using-the-default-terminal-heres-the-setup-you-deserve-bf5e52b51e5d?source=post_page---read_next_recirc--3be6ea745f0f----2---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)

![Using a Local LLM in Obsidian with LM Studio](https://miro.medium.com/v2/resize:fit:1358/format:webp/1*LsRMZOHC7L0K71wzlQ6__g.png)[![Alex Carter](https://miro.medium.com/v2/resize:fill:40:40/1*t1UDIKh4IY6FOXOQ_7fcPQ.png)](/@iamalexcarter?source=post_page---read_next_recirc--3be6ea745f0f----3---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Alex Carter](/@iamalexcarter?source=post_page---read_next_recirc--3be6ea745f0f----3---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)[Using a Local LLM in Obsidian with LM Studio
--------------------------------------------

### This article explains how to run a local large language model (LLM) in Obsidian using LM Studio.](/@iamalexcarter/using-a-local-llm-in-obsidian-with-lm-studio-c5855f02e88c?source=post_page---read_next_recirc--3be6ea745f0f----3---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)Jun 4[4](/@iamalexcarter/using-a-local-llm-in-obsidian-with-lm-studio-c5855f02e88c?source=post_page---read_next_recirc--3be6ea745f0f----3---------------------26b88b12_7615_4f93_90f0_c7485b805df8--------------)

[See more recommendations](/?source=post_page---read_next_recirc--3be6ea745f0f---------------------------------------)[Help](https://help.medium.com/hc/en-us?source=post_page-----3be6ea745f0f---------------------------------------)[Status](https://status.medium.com/?source=post_page-----3be6ea745f0f---------------------------------------)[About](/about?autoplay=1&source=post_page-----3be6ea745f0f---------------------------------------)[Careers](/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3be6ea745f0f---------------------------------------)[Press](mailto:pressinquiries@medium.com)[Blog](https://blog.medium.com/?source=post_page-----3be6ea745f0f---------------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3be6ea745f0f---------------------------------------)[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----3be6ea745f0f---------------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3be6ea745f0f---------------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----3be6ea745f0f---------------------------------------)[https://www.google.com/recaptcha/enterprise/anchor?ar=1&k=6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp&co=aHR0cHM6Ly9tZWRpdW0uY29tOjQ0Mw..&hl=en&v=bGi-DxR800FVc7f0siDI2jNQ&size=invisible&anchor-ms=20000&execute-ms=15000&cb=fxzvzsaa00qz](https://www.google.com/recaptcha/enterprise/anchor?ar=1&k=6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp&co=aHR0cHM6Ly9tZWRpdW0uY29tOjQ0Mw..&hl=en&v=bGi-DxR800FVc7f0siDI2jNQ&size=invisible&anchor-ms=20000&execute-ms=15000&cb=fxzvzsaa00qz)

✅ Full page copied to clipboard!